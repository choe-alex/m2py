{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calculations of loss during the training of networks can use weighting to reinforce the learning of specific variables and features in data. In order to reinforce physically reasonable predictions, you can impose more strict penalties for predictions in physically invalid predictions. For example, a prediction of Power Conversion Efficiency (PCE) above the Schockley-Quassier predicted maximum PCE could increase exponentially, rather than linearly.\n",
    "\n",
    "The classes developed below are wrappers for PyTorch tensors loss functions, which additionally modify these classes using theoretically and emprically derived boundaries for network loss calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCE_Loss(torch.autograd.Function()):\n",
    "    \"\"\"\n",
    "    This class contains loss functions that use a mean-squared-error loss for reasonable predictions.\n",
    "    They inherit from torch.nn.Module just like the custom model. For physically unreasonable conditions,\n",
    "    prediction loss is more severely calculated. What qualifies as reasonable is based on empirically\n",
    "    gathered datasets and literature reported boundaries of performance in P3HT:PCBM OPV devices.\n",
    "    \n",
    "    For the following Power Conversion Efficiency predictions that are improbable, the loss is penalized:\n",
    "    - PCE < 0%\n",
    "    - PCE > 6%\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(PCE_Loss, self).__init()\n",
    "\n",
    "    @staticmethod   \n",
    "    def forward(ctx, predictions, labels):\n",
    "        for x, y in predictions, labels:\n",
    "\n",
    "            if x < 0:\n",
    "                #Exponential MSE\n",
    "                result = F.mse_loss(x,y)\n",
    "                result = torch.pow(result, 2)\n",
    "\n",
    "\n",
    "            elif x > 6:\n",
    "                #exponential MSE\n",
    "                result = F.mse_loss(x,y)\n",
    "                result = result.pow(loss, 2)\n",
    "\n",
    "            else:\n",
    "                result = F.mse_loss()\n",
    "\n",
    "            #tell contex object to save operation tensor for autograd.backward\n",
    "            ctx.save_for_backward(result)\n",
    "\n",
    "\n",
    "#                 result.requires_grad = True\n",
    "#                 result.retain_grad()\n",
    "\n",
    "            return result\n",
    "\n",
    "    @staticmethod        \n",
    "    def backward(ctx, grad_output):\n",
    "\n",
    "        result, weight, bias = ctx.saved_tensors\n",
    "\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input = grad_output.mm(weight)\n",
    "\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_weight = grad_output.t().mm(input)\n",
    "\n",
    "        if bias is not None and ctx.needs_input_grad[2]:\n",
    "            grad_bias = grad_output.sum(0).squeeze(0)\n",
    "\n",
    "\n",
    "\n",
    "        return grad_input, grad_weight, grad_bias\n",
    "\n",
    "\n",
    "class Voc_Loss(torch.autograd.Function()):\n",
    "    \"\"\"\n",
    "    This class contains loss functions that use a mean-squared-error loss for reasonable predictions.\n",
    "    They inherit from torch.nn.Module just like the custom model. For physically unreasonable conditions,\n",
    "    prediction loss is more severely calculated. What qualifies as reasonable is based on empirically\n",
    "    gathered datasets and literature reported boundaries of performance in P3HT:PCBM OPV devices.\n",
    "    \n",
    "     For the following open-circuitt voltage predictions that are improbable, the loss is penalized:\n",
    "    - Voc < 0\n",
    "    - Voc > 1.0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Voc_Loss, self).__init()\n",
    "\n",
    "    @staticmethod   \n",
    "    def forward(ctx, predictions, labels):\n",
    "        for x, y in predictions, labels:\n",
    "\n",
    "            if x < 0:\n",
    "                #Exponential MSE\n",
    "                result = F.mse_loss(x,y)\n",
    "                result = torch.pow(result, 2)\n",
    "\n",
    "\n",
    "            elif x > 1.0:\n",
    "                #exponential MSE\n",
    "                result = F.mse_loss(x,y)\n",
    "                result = result.pow(loss, 2)\n",
    "\n",
    "            else:\n",
    "                result = F.mse_loss()\n",
    "\n",
    "            #tell contex object to save operation tensor for autograd.backward\n",
    "            ctx.save_for_backward(result)\n",
    "\n",
    "\n",
    "#                 result.requires_grad = True\n",
    "#                 result.retain_grad()\n",
    "\n",
    "            return result\n",
    "\n",
    "    @staticmethod        \n",
    "    def backward(ctx, grad_output):\n",
    "\n",
    "        result, weight, bias = ctx.saved_tensors\n",
    "\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input = grad_output.mm(weight)\n",
    "\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_weight = grad_output.t().mm(input)\n",
    "\n",
    "        if bias is not None and ctx.needs_input_grad[2]:\n",
    "            grad_bias = grad_output.sum(0).squeeze(0)\n",
    "\n",
    "\n",
    "        return grad_input, grad_weight, grad_bias\n",
    "\n",
    "\n",
    "class Jsc_Loss(torch.autograd.Function()):\n",
    "    \"\"\"\n",
    "    This class contains loss functions that use a mean-squared-error loss for reasonable predictions.\n",
    "    They inherit from torch.nn.Module just like the custom model. For physically unreasonable conditions,\n",
    "    prediction loss is more severely calculated. What qualifies as reasonable is based on empirically\n",
    "    gathered datasets and literature reported boundaries of performance in P3HT:PCBM OPV devices.\n",
    "    \n",
    "     For the following short-circuit current predictions that are improbable, the loss is penalized:\n",
    "    - Jsc < 0\n",
    "    - Jsc > 10\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Jsc_Loss, self).__init()\n",
    "\n",
    "    @staticmethod   \n",
    "    def forward(ctx, predictions, labels):\n",
    "        for x, y in predictions, labels:\n",
    "\n",
    "            if x < 0:\n",
    "                #Exponential MSE\n",
    "                result = F.mse_loss(x,y)\n",
    "                result = torch.pow(result, 2)\n",
    "\n",
    "\n",
    "            elif x > 10:\n",
    "                #exponential MSE\n",
    "                result = F.mse_loss(x,y)\n",
    "                result = result.pow(loss, 2)\n",
    "\n",
    "            else:\n",
    "                result = F.mse_loss()\n",
    "\n",
    "            #tell contex object to save operation tensor for autograd.backward\n",
    "            ctx.save_for_backward(result)\n",
    "\n",
    "\n",
    "#                 result.requires_grad = True\n",
    "#                 result.retain_grad()\n",
    "\n",
    "            return result\n",
    "\n",
    "    @staticmethod        \n",
    "    def backward(ctx, grad_output):\n",
    "\n",
    "        result, weight, bias = ctx.saved_tensors\n",
    "\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input = grad_output.mm(weight)\n",
    "\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_weight = grad_output.t().mm(input)\n",
    "\n",
    "        if bias is not None and ctx.needs_input_grad[2]:\n",
    "            grad_bias = grad_output.sum(0).squeeze(0)\n",
    "\n",
    "\n",
    "\n",
    "        return grad_input, grad_weight, grad_bias\n",
    "\n",
    "class FF_Loss(torch.autograd.Function()):\n",
    "    \"\"\"\n",
    "    This class contains loss functions that use a mean-squared-error loss for reasonable predictions.\n",
    "    They inherit from torch.nn.Module just like the custom model. For physically unreasonable conditions,\n",
    "    prediction loss is more severely calculated. What qualifies as reasonable is based on empirically\n",
    "    gathered datasets and literature reported boundaries of performance in P3HT:PCBM OPV devices.\n",
    "    \n",
    "     For the following Fill Factor predictions that are improbable, the loss is penalized:\n",
    "    - FF < 10\n",
    "    - FF > 85\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FF_Loss, self).__init()\n",
    "\n",
    "    @staticmethod   \n",
    "    def forward(ctx, predictions, labels):\n",
    "        for x, y in predictions, labels:\n",
    "\n",
    "            if x < 10:\n",
    "                #Exponential MSE\n",
    "                result = F.mse_loss(x,y)\n",
    "                result = torch.pow(result, 2)\n",
    "\n",
    "\n",
    "            elif x > 85:\n",
    "                #exponential MSE\n",
    "                result = F.mse_loss(x,y)\n",
    "                result = result.pow(loss, 2)\n",
    "\n",
    "            else:\n",
    "                result = F.mse_loss()\n",
    "\n",
    "            #tell contex object to save operation tensor for autograd.backward\n",
    "            ctx.save_for_backward(result)\n",
    "\n",
    "\n",
    "#                 result.requires_grad = True\n",
    "#                 result.retain_grad()\n",
    "\n",
    "            return result\n",
    "\n",
    "    @staticmethod        \n",
    "    def backward(ctx, grad_output):\n",
    "\n",
    "        result, weight, bias = ctx.saved_tensors\n",
    "\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input = grad_output.mm(weight)\n",
    "\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_weight = grad_output.t().mm(input)\n",
    "\n",
    "        if bias is not None and ctx.needs_input_grad[2]:\n",
    "            grad_bias = grad_output.sum(0).squeeze(0)\n",
    "\n",
    "        return grad_input, grad_weight, grad_bias"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
